{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b19d92",
   "metadata": {},
   "source": [
    " 가상환경 삭제 : conda remove -n 가상환경이름 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b6f08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.6/151.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.7.25-cp39-cp39-win_amd64.whl (262 kB)\n",
      "     -------------------------------------- 262.8/262.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\medici\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tokenizers, tqdm, regex, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.1 huggingface-hub-0.8.1 pyyaml-6.0 regex-2022.7.25 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c4a9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.20.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2b2cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch # pytorch install 해야함\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFGPT2Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137ed9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2728ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train, test_size=0.25, \n",
    "                                        stratify = train.target,\n",
    "                                        random_state =0) # 25프로로 설정\n",
    "# 전처리 과정에서 데이터가 뒤섞이지 않도록 인덱스를 초기화\n",
    "train_data = train_data.reset_index().drop('index', axis=1)\n",
    "val_data = val_data.reset_index().drop('index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0750a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2',bos_token='</s>',eos_token='</s>',pad_token='<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf7f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25906,\n",
       " 8702,\n",
       " 7801,\n",
       " 25856,\n",
       " 9265,\n",
       " 7162,\n",
       " 16071,\n",
       " 15460,\n",
       " 11510,\n",
       " 10973,\n",
       " 7898,\n",
       " 8135,\n",
       " 9212,\n",
       " 7791,\n",
       " 8092,\n",
       " 9241,\n",
       " 7172,\n",
       " 7182]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('안녕하세요. 저는 데이터분석가가 되고싶은 최선우 입니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93888e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁안녕',\n",
       " '하',\n",
       " '세',\n",
       " '요.',\n",
       " '▁저',\n",
       " '는',\n",
       " '▁데이터',\n",
       " '분석',\n",
       " '가가',\n",
       " '▁되고',\n",
       " '싶',\n",
       " '은',\n",
       " '▁최',\n",
       " '선',\n",
       " '우',\n",
       " '▁입',\n",
       " '니',\n",
       " '다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('안녕하세요. 저는 데이터분석가가 되고싶은 최선우 입니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cf5b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요. 저는 데이터분석가가 되고싶은 최선우 입니다'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('안녕하세요. 저는 데이터분석가가 되고싶은 최선우 입니다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac820eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 140 # 제일 긴 문장의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b8eaebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25906,\n",
       " 8702,\n",
       " 7801,\n",
       " 25856,\n",
       " 9265,\n",
       " 7162,\n",
       " 16071,\n",
       " 15460,\n",
       " 11510,\n",
       " 10973,\n",
       " 7898,\n",
       " 8135,\n",
       " 9212,\n",
       " 7791,\n",
       " 8092,\n",
       " 9241,\n",
       " 7172,\n",
       " 7182,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('안녕하세요. 저는 데이터분석가가 되고싶은 최선우 입니다',max_length=max_seq_len,pad_to_max_length = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970894e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, data_labels = [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "\n",
    "        bos_token = [tokenizer.bos_token]\n",
    "        eos_token = [tokenizer.eos_token]\n",
    "        tokens = bos_token + tokenizer.tokenize(example) + eos_token\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_id = pad_sequences([input_id], maxlen=max_seq_len, value=tokenizer.pad_token_id, padding='post')[0]\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        input_ids.append(input_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return input_ids, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8845fa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10749</td>\n",
       "      <td>심플하면서 깔끔하고 가격 저렴하고 배송 빠르고~ 최고네요 ^^</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12258</td>\n",
       "      <td>재구매 아주 좋아요 손톱관리하기가 너무 좋아요~~</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16351</td>\n",
       "      <td>조립도 쉽고 디자인 너무 예뻐요ㅎㅎ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9402</td>\n",
       "      <td>재구매 배송 빠르고 불량품 없어서 좋아요</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7722</td>\n",
       "      <td>배송은 늦었지만 수령후 설치해서사용중인데 모니터 고정부분이 움직일때 좌우로 움직이네...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6245</th>\n",
       "      <td>24390</td>\n",
       "      <td>싸서샀는데별로임 질떨어짐</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6246</th>\n",
       "      <td>16848</td>\n",
       "      <td>잘 받아 보았습니다</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6247</th>\n",
       "      <td>20355</td>\n",
       "      <td>배송이 너무 늦게오네요..</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6248</th>\n",
       "      <td>15497</td>\n",
       "      <td>향이...별로예요 ㅠㅠㅠ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>9340</td>\n",
       "      <td>가성비갑 흠집없는 제품 와서 만족</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            reviews  target\n",
       "0     10749                 심플하면서 깔끔하고 가격 저렴하고 배송 빠르고~ 최고네요 ^^       5\n",
       "1     12258                        재구매 아주 좋아요 손톱관리하기가 너무 좋아요~~       5\n",
       "2     16351                                조립도 쉽고 디자인 너무 예뻐요ㅎㅎ       5\n",
       "3      9402                             재구매 배송 빠르고 불량품 없어서 좋아요       5\n",
       "4      7722  배송은 늦었지만 수령후 설치해서사용중인데 모니터 고정부분이 움직일때 좌우로 움직이네...       4\n",
       "...     ...                                                ...     ...\n",
       "6245  24390                                      싸서샀는데별로임 질떨어짐       2\n",
       "6246  16848                                         잘 받아 보았습니다       2\n",
       "6247  20355                                     배송이 너무 늦게오네요..       2\n",
       "6248  15497                                      향이...별로예요 ㅠㅠㅠ       1\n",
       "6249   9340                                 가성비갑 흠집없는 제품 와서 만족       5\n",
       "\n",
       "[6250 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70764bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 18750/18750 [00:02<00:00, 7699.38it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = convert_examples_to_features(train_data['reviews'], train_data['target'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185e64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6250/6250 [00:00<00:00, 7705.41it/s]\n"
     ]
    }
   ],
   "source": [
    "val_X, val_y = convert_examples_to_features(val_data['reviews'], val_data['target'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb46456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> 상품이 저가 원하는대로 왔어요.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3c1965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2Model: ['lm_head.weight', 'transformer.h.2.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.5.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2Model.from_pretrained('skt/kogpt2-base-v2', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aec804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc3db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = tf.keras.layers.Input(shape=(max_seq_len),dtype=tf.int32)\n",
    "outputs = model([layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5f3863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFGPT2ForSequenceClassification(tf.keras.Model):\n",
    "    def __init__(self, model_name):\n",
    "        super(TFGPT2ForSequenceClassification, self).__init__()\n",
    "        self.gpt = TFGPT2Model.from_pretrained(model_name, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.classifier = tf.keras.layers.Dense(4,\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n",
    "                                                activation='softmax',\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.gpt(input_ids=inputs)\n",
    "        cls_token = outputs[0][:, -1]\n",
    "        cls_token = self.dropout(cls_token)\n",
    "        prediction = self.classifier(cls_token)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed02ab6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'COLAB_TPU_ADDR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40452/3521482683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grpc://'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;31m# raise KeyError with the original key value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
